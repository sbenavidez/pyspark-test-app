{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark example for Santiago\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+------------+\n",
      "|      Data|    Time|Transaction|        Item|\n",
      "+----------+--------+-----------+------------+\n",
      "|2016-10-30|09:58:11|          1|       Bread|\n",
      "|2016-10-30|10:05:34|          2|Scandinavian|\n",
      "|2016-10-30|10:05:34|          2|Scandinavian|\n",
      "+----------+--------+-----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prueba con header false\n",
    "schema = StructType([\n",
    "    StructField(\"Data\", StringType()),\n",
    "    StructField(\"Time\", StringType()),\n",
    "    StructField(\"Transaction\", IntegerType()),\n",
    "    StructField(\"Item\", StringType())\n",
    "])\n",
    "\n",
    "peopleDF = spark.read.csv('/home/sbenavidez/work/testDataset.csv', header=False, schema=schema)\n",
    "\n",
    "peopleDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|         Description|  Item|\n",
      "+--------------------+------+\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|                Jale|   Jam|\n",
      "| Muffins de choco...|Muffin|\n",
      "|               cafe |Coffee|\n",
      "|        Pan caliente| Bread|\n",
      "| Muffins de choco...|Muffin|\n",
      "|               cafe |Coffee|\n",
      "|         Te de boldo|   Tea|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "| Muffins de choco...|Muffin|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "|                Jale|   Jam|\n",
      "|               cafe |Coffee|\n",
      "|         Te de boldo|   Tea|\n",
      "|        Pan caliente| Bread|\n",
      "|               cafe |Coffee|\n",
      "|        Pan caliente| Bread|\n",
      "|        Pan caliente| Bread|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prueba varias\n",
    "\n",
    "# Defino schema para cada uno\n",
    "schematable1 = StructType([\n",
    "    StructField(\"Data\", StringType()),\n",
    "    StructField(\"Time\", StringType()),\n",
    "    StructField(\"Transaction\", IntegerType()),\n",
    "    StructField(\"Item\", StringType())\n",
    "])\n",
    "\n",
    "schematable2 = StructType([\n",
    "    StructField(\"Item\", StringType()),\n",
    "    StructField(\"Description\", StringType())\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.read.csv('/home/sbenavidez/work/BreadBasket_DMS.csv', header=True, schema=schematable1)\n",
    "df2 = spark.read.csv('/home/sbenavidez/work/testDataset.csv', header=True, schema=schematable2)\n",
    "\n",
    "#Mostrar data\n",
    "#df.show(3)\n",
    "\n",
    "#Print schema\n",
    "#df.printSchema()\n",
    "\n",
    "#Print una table especifica\n",
    "#df.select(\"Data\").show(3)\n",
    "#df.select(\"Data\",\"Item\").show(3)\n",
    "\n",
    "#Print de un where\n",
    "#df.select(\"Data\",df.Item.like(\"Bread\")).show(3)\n",
    "#df.select(df['Data']).show(3)\n",
    "#df.filter(df['Transaction']<2).show()\n",
    "\n",
    "#Probando group by y order by \n",
    "#df.groupBy(\"Item\").count().orderBy([\"Item\"],descending=[0,1]).show(10)\n",
    "\n",
    "#Probando strings\n",
    "#df.select(\"Data\",df.Item.startswith(\"Bread\"),\"Item\").show()\n",
    "\n",
    "#Haciendo un filtrado y select \n",
    "#df.select(\"Item\",\"Data\",\"Transaction\").filter(df[\"Transaction\"]>24).show(4)\n",
    "\n",
    "#df.write.save(\"/outputTest.csv\",format=\"csv\")\n",
    "\n",
    "#df.select(\"Item\",\"Data\",\"Transaction\").filter(df[\"Transaction\"]>24)\\\n",
    "   # .write.save(\"outputTest2.csv\",format=\"csv\")\n",
    "\n",
    "# Probar Spark SQL\n",
    "#df.createOrReplaceTempView(\"BreadBasket_DMS\")\n",
    "\n",
    "#sqlDF = spark.sql(\"SELECT * FROM BreadBasket_DMS\")\n",
    "#sqlDF.show(5)\n",
    "\n",
    "# Show de la segunda tabla\n",
    "#df2.show()\n",
    "\n",
    "# Join tables\n",
    "#queryResult = df.join(df2,df.Item == df2.Item,'inner').select(df.Item,df2.Description).limit(20)\n",
    "\n",
    "# Pruebas de guardado\n",
    "#queryResult.write.save(\"outputFileJoin.csv\",formta=\"csv\")\n",
    "\n",
    "#queryResult.write.save(\"outputFileJoin2.csv\")\n",
    "\n",
    "#parquetDF = spark.read.load(\"outputFileJoin2.csv\",format=\"parquet\")\n",
    "#parquetDF.show()\n",
    "\n",
    "#jsonDF = queryResult.write.save(\"outputFileJoin3.json\",format=\"json\")\n",
    "\n",
    "#Prueba de append de files\n",
    "#jsonDF = queryResult.write.save(\"outputFileJoin4.json\",format=\"json\",mode=\"append\")\n",
    "\n",
    "jsonDF = spark.read.load(\"outputFileJoin4.json\",format=\"json\")\n",
    "jsonDF.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
